{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Web Crawler-BFS and DFS.ipynb","provenance":[],"authorship_tag":"ABX9TyPVWPobaXh3a+P4SLO/sDKn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"3T2WF3eWNLjw"},"source":["#BFS Crawler\n","import requests\n","from bs4 import BeautifulSoup\n","from collections import deque\n","\n","visited = set([\"http://toscrape.com\"])\n","dq = deque([[\"http://toscrape.com\", \"\", 0]])\n","max_depth = 3\n","\n","while dq:\n","    base, path, depth = dq.popleft()\n","    \n","    if depth < max_depth:\n","        try:\n","            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n","\n","            for link in soup.find_all(\"a\"):\n","                href = link.get(\"href\")\n","\n","                if href not in visited:\n","                    visited.add(href)\n","                    print(\"  \" * depth + f\"at depth {depth}: {href}\")\n","\n","                    if href.startswith(\"http\"):\n","                        dq.append([href, \"\", depth + 1])\n","                    else:\n","                        dq.append([base, href, depth + 1])\n","        except:\n","            pass\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6r6EscVnho9m"},"source":["#DFS Crawler\n","import requests\n","from bs4 import BeautifulSoup\n","\n","def get_links_recursive(base, path, visited, max_depth=3, depth=0):\n","    if depth < max_depth:\n","        try:\n","            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n","\n","            for link in soup.find_all(\"a\"):\n","                href = link.get(\"href\")\n","\n","                if href not in visited:\n","                    visited.add(href)\n","                    print(f\"at depth {depth}: {href}\")  \n","\n","                    if href.startswith(\"http\"):\n","                        get_links_recursive(href, \"\", visited, max_depth, depth + 1)\n","                    else:\n","                        get_links_recursive(base, href, visited, max_depth, depth + 1)\n","        except:\n","            pass\n","\n","\n","get_links_recursive(\"http://toscrape.com\", \"\", set([\"http://toscrape.com\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOakNPe1NU8f"},"source":["from bs4 import BeautifulSoup\n","import requests\n","\n","pages_crawled = []\n","\n","\n","def crawler(url):\n","    page = requests.get(url)\n","    soup = BeautifulSoup(page.text, 'html.parser')\n","    links = soup.find_all('a')\n","\n","    for link in links:\n","        if 'href' in link.attrs:\n","            if link['href'].startswith('/wiki') and ':' not in link['href']:\n","                if link['href'] not in pages_crawled:\n","                    new_link = f\"https://en.wikipedia.org{link['href']}\"\n","                    pages_crawled.append(link['href'])\n","                    try:\n","                        with open('data.csv', 'a') as file:\n","                            file.write(f'{soup.title.text}; {soup.h1.text}; {link[\"href\"]}\\n')\n","                        crawler(new_link)\n","                    except:\n","                        continue\n","                                       \n","                                                                 \n","crawler('https://en.wikipedia.org')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqvM8SABdZzo"},"source":[""],"execution_count":null,"outputs":[]}]}